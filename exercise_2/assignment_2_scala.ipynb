{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://LAPTOP-PC8F3R05:4041\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1587886592235)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.json4s._\r\n",
       "import org.json4s.jackson.JsonMethods._\r\n",
       "import scala.runtime.ScalaRunTime._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.json4s._\n",
    "import org.json4s.jackson.JsonMethods._\n",
    "import scala.runtime.ScalaRunTime._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file_path: String = D:\\TU WIEN\\DIC SS20\\Assignment_2\\reviews_devset.json\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file_path = \"D:\\\\TU WIEN\\\\DIC SS20\\\\Assignment_2\\\\reviews_devset.json\"\n",
    "//val peopleDF = spark.read.json(path)\n",
    "//peopleDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names of the columns:\n",
    "\n",
    "root <br>\n",
    " |-- asin: string (nullable = true) <br>\n",
    " |-- category: string (nullable = true) <br>\n",
    " |-- helpful: array (nullable = true) <br>\n",
    " |    |-- element: long (containsNull = true) <br>\n",
    " |-- overall: double (nullable = true) <br>\n",
    " |-- reviewText: string (nullable = true) <br>\n",
    " |-- reviewTime: string (nullable = true) <br>\n",
    " |-- reviewerID: string (nullable = true) <br>\n",
    " |-- reviewerName: string (nullable = true) <br>\n",
    " |-- summary: string (nullable = true) <br>\n",
    " |-- unixReviewTime: long (nullable = true) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[String] = D:\\TU WIEN\\DIC SS20\\Assignment_2\\reviews_devset.json MapPartitionsRDD[1] at textFile at <console>:36\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.textFile(file_path)\n",
    "\n",
    "/* Works:\n",
    "rdd.map{ row =>\n",
    "  val json_row = parse(row)\n",
    "\n",
    "  (compact(json_row \\ \"asin\") + \"test\", compact(json_row \\ \"reviewText\"))\n",
    "}.collect().foreach{println _}\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd_text: org.apache.spark.rdd.RDD[(String, String, String)] = MapPartitionsRDD[2] at map at <console>:35\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_text = rdd.map{ row =>\n",
    "      val json_row = parse(row)\n",
    "\n",
    "      (compact(json_row \\ \"reviewText\"), compact(json_row \\ \"category\"), compact(json_row \\ \"asin\") + compact(json_row \\ \"reviewerID\"))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Array(This, was, gift, for, my, other, husband, He's, making, us, things, from, it, all, the, time, and, we, love, the, food, Directions, are, simple, easy, to, read, and, interpret, and, fun, to, make, We, all, love, different, kinds, of, cuisine, and, Raichlen, provides, recipes, from, everywhere, along, the, barbecue, trail, as, he, calls, it, Get, it, and, just, open, page, Have, at, it, You'll, love, the, food, and, it, has, provided, us, with, an, insight, into, the, culture, that, produced, it, It's, all, about, broadening, horizons, Yum),\"Patio_Lawn_and_Garde\",\"0981850006\"\"A2VNYWOPJ13AFP\")\n",
      "(Array(This, is, very, nice, spreader, It, feels, very, solid, and, the, pneumatic, tires, give, it, great, maneuverability, and, handling, over, bumps, The, control, arm, is, solid, metal, not, cable, which, gives, you, precise, control, and, will, last, long, time, The, settings, take, some, experimentation, with, your, various, products, to, get, it, right, but, that, is, true, of, any, spreader, It, has, good, distribution, probably, flings, material, little, farther, on, the, right, side, than, the, left, but, it, is, far, more, even, than, my, crappy, Edgeguard, ever, was),\"Patio_Lawn_and_Garde\",\"B00002N66D\"\"A2E5XXXC07AGA7\")\n",
      "(Array(The, metal, base, with, the, hose, attachments, is, very, poorly, designed, and, made, As, the, previous, reviewer, pointed, out, it, leaks, badly, and, just, pops, off, There, is, no, way, to, fix, or, tighten, this, junction, and, when, the, plastic, wears, bit, the, hose, just, falls, off, and, sprinkler, is, useless, AVOID, ANY, GILMOUR, Sprinklers, with, this, spike, base, The, sprinkler, heads, themselves, work, fairly, well),\"Patio_Lawn_and_Garde\",\"B00002N67U\"\"A16PX63WZIEQ13\")\n",
      "(Array(For, the, most, part, this, works, pretty, good, bought, the, John, Deere, and, this, one, they, both, work, the, same, This, is, the, first, time, am, waterin, my, lawn, so, to, me, it, seems, like, they, use, lot, of, water, especially, in, narrow, areas, They, are, better, made, for, large, areas, for, narrow, corridors, use, some, other, system),\"Patio_Lawn_and_Garde\",\"B00002N6AN\"\"A2OSWM3522VARA\")\n",
      "(Array(This, hose, is, supposed, to, be, flexible, Its, hard, heavy, and, unwieldy, don't, know, if, it, kinks, or, not, because, could, tell, as, soon, as, removed, it, from, the, box, that, would, have, to, return, it, If, you, want, lightweight, soft, hose, and, know, they, exist, because, have, one, do, not, buy, this, thing),\"Patio_Lawn_and_Garde\",\"B00002N8K3\"\"A2SX9YPPGEUADI\")\n",
      "(Array(This, tool, works, very, well, for, cutting, brush, and, woody, stemmed, weeds, up, to, about, 34, It, will, work, for, larger, brush, and, small, trees, with, multiple, swings, but, find, it, easier, to, use, bow, saw, for, the, occasional, larger, plants, encounter, Note, that, if, you, need, to, cut, lot, of, larger, brush, brush, hook, would, be, better, than, this, ditch, bank, blade, as, the, brush, hook, blade, is, thicker, and, heavier, The, tool, needs, to, be, sharpened, right, out, of, the, package, to, perform, best, This, removes, the, paint, and, any, nicks, and, burrs, from, the, factory, grinding, as, well, shipping, and, handling, nicks, resharpen, the, blades, and, spray, them, with, WD, 40, after, each, use, so, it, remains, rust, free, and, is, ready, to, go, for, the, next, session, of, cutting, brush, Also, suggest, installing, either, lock, washers, or, nyloc, nuts, on, the, bolts, to, keep, them, tight, in, use, Even, with, that, check, them, after, each, use, to, make, sure, they, are, tight, it, seems, that, initially, the, ash, handle, compresses, bit, and, the, metal, side, plates, need, time, to, seat, fully, into, the, ash, This, tool, has, been, huge, improvement, over, my, large, lopping, shears, and, machette, for, clearing, brush, on, new, rail, trail, Great, tool, if, used, appropriately, Per, the, package, it, can, also, be, used, to, ward, off, zombies, and, the, undead, but, I've, not, had, that, need, to, date),\"Patio_Lawn_and_Garde\",\"B00002NBQ8\"\"A2PENG0PDZUEGN\")\n",
      "(Array(This, product, is, typical, yet, usable, design, The, handle, mounted, power, cord, lock, is, somewhat, difficult, to, use, but, works, fairly, well, after, the, cord, is, locked, into, type, of, holder, that, could, be, improved, The, product, does, what, it, is, intended, to, do, no, more, no, less, It, is, powerful, vacuum, and, works, very, well, with, high, amounts, of, dry, leaf, material, The, attached, bagger, is, somewhat, cumbersome, to, use, and, empty, produces, copious, amounts, of, leaf, dust, but, overall, this, product, works, quite, well, in, vacuuming, lots, of, leaf, and, yard, material),\"Patio_Lawn_and_Garde\",\"B00004DTNG\"\"A2NBUMLJ0QBTND\")\n",
      "(Array(was, excited, to, ditch, my, 99, two, prong, corn, holders, for, this, corkscrew, design, but, regret, it, The, corkscrew, design, twists, into, the, end, of, the, corn, thought, this, was, genius, idea, but, the, problem, is, that, the, prong, continues, to, twist, when, in, the, corn, Since, it, doesn't, have, second, prong, to, prevent, the, pivoting, the, corn, will, flip, and, slip, all, around, the, prong, which, is, super, annoying, when, trying, to, butter, and, eat, The, storage, case, and, grips, are, nice, but, think, the, design, itself, is, problematic, I'll, be, ditching, these, and, going, back, to, two, prong, approach),\"Patio_Lawn_and_Garde\",\"B00004OCJZ\"\"AC9EDJLYU6DEH\")\n",
      "(Array(purchased, the, Leaf, Hog, last, year, and, was, very, impressed, with, it, It, helped, reduce, the, time, to, take, care, of, leaves, from, about, 80, hours, of, work, to, about, 35, hours, and, my, back, didn't, hurt, nearly, as, much, last, year, This, year, tried, out, the, Leaf, Collection, system, as, much, of, the, time, spent, last, year, was, dumping, out, the, leaf, collection, bag, to, mulch, more, until, the, garbage, can, was, full, On, this, go, around, had, 55, gallon, garbage, cans, and, started, mulching, the, big, piles, had, would, just, mulch, until, can, was, full, move, the, fabric, over, to, the, next, can, and, continue, This, sped, up, my, time, to, finishing, in, under, 20, hours, this, year, would, highly, recommend, it, if, you, need, to, mulch, more, than, single, bag, full, It, saved, ton, of, time, for, me, and, have, no, doubt, it, can, do, the, same, for, you),\"Patio_Lawn_and_Garde\",\"B00004R9TJ\"\"A2OKI2AQ4QQV8R\")\n",
      "(Array(Never, used, manual, lawnmower, Need, to, adjust, heigth, of, back, roller, If, you, have, small, area, then, its, fine, You, will, need, trimmer, though, for, edges, and, difficult, areas, May, have, to, go, over, the, area, twice, to, make, the, cut, even),\"Patio_Lawn_and_Garde\",\"B00004R9UK\"\"A1DYWRLW6ZB4FW\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd_text_split: org.apache.spark.rdd.RDD[(Array[String], String, String)] = MapPartitionsRDD[3] at map at <console>:39\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* With array \n",
    "val rdd_text_split = rdd_text.map(x => x.split(Array('.' , ' ', '?', '!', ',', ';', ':', '(', ')', '[',']','{','}','-','_',\n",
    "                                                    '`','~','#','&','*','%','$')))\n",
    "*/\n",
    "val rdd_text_split = rdd_text.map(x =>(\n",
    "\"\\\\s+|\\\\,|\\\\\\\\|/|\\\\.|\\\\!|\\\\?|\\\\;|\\\\:|\\\\(|\\\\)|\\\\[|\\\\]|\\\\{|\\\\}|\\\\-|\\\\_|\\\"|\\\\`|\\\\~|\\\\#|\\\\&|\\\\*|\\\\%|\\\\$\".r.split(x._1).filter(y => y.length > 1), x._2, x._3))\n",
    "\n",
    "rdd_text_split.take(10).foreach(array => println(stringOf(array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd_text_no_duplicates: org.apache.spark.rdd.RDD[(List[String], String, String)] = MapPartitionsRDD[4] at map at <console>:35\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_text_no_duplicates = rdd_text_split.map(x => (x._1.toSet.toList, x._2,x._3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(List(barbecue, read, broadening, for, recipes, raichlen, insight, provided, are, it's, you'll, produced, this, different, get, all, he's, trail, page, gift, horizons, just, us, it, calls, provides, as, have, has, we, husband, open, culture, cuisine, that, to, making, was, at, kinds, my, directions, everywhere, easy, food, simple, things, he, love, with, from, fun, make, an, into, time, about, we, along, yum, other, of, and, interpret, the),\"Patio_Lawn_and_Garde\",\"0981850006\"\"A2VNYWOPJ13AFP\")\n",
      "(List(side, maneuverability, any, your, is, than, experimentation, solid, this, but, good, distribution, handling, pneumatic, left, it, cable, has, crappy, nice, products, feels, far, last, flings, that, arm, to, you, probably, various, was, long, gives, the, over, on, my, precise, take, spreader, farther, even, will, little, not, with, give, edgeguard, true, it, which, control, material, get, time, more, metal, very, some, great, of, settings, and, right, ever, tires, bumps, the),\"Patio_Lawn_and_Garde\",\"B00002N66D\"\"A2E5XXXC07AGA7\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd_text_casefolding: org.apache.spark.rdd.RDD[(List[String], String, String)] = MapPartitionsRDD[5] at map at <console>:35\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_text_casefolding = rdd_text_no_duplicates.map(x => (x._1.map(_.toLowerCase()),x._2,x._3))\n",
    "rdd_text_casefolding.take(2).foreach(array => println(stringOf(array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.io.Source\r\n",
       "import scala.collection.mutable.ListBuffer\r\n",
       "stopWordsHelper: scala.collection.mutable.ListBuffer[String] = ListBuffer(a's, able, about, above, according, accordingly, across, actually, after, afterwards, again, against, ain't, all, allow, allows, almost, alone, along, already, also, although, always, am, among, amongst, an, and, another, any, anybody, anyhow, anyone, anything, anyway, anyways, anywhere, apart, appear, appreciate, appropriate, are, aren't, around, as, aside, ask, asking, associated, at, available, away, awfully, be, became, because, become, becomes, becoming, been, before, beforehand, behind, being, believe, below, beside, besides, best, better, between, beyond, both, brief, but, by, c'mon, c's, came, can, can't, cannot, cant, cause, causes, certa..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//creating stopWords List:\n",
    "import scala.io.Source\n",
    "import scala.collection.mutable.ListBuffer\n",
    "\n",
    "var stopWordsHelper = new ListBuffer[String]()\n",
    "val filename = \"stopwords.txt\"\n",
    "for (line <- Source.fromFile(filename).getLines) {\n",
    "    stopWordsHelper += line\n",
    "}\n",
    "val stopWords = stopWordsHelper.toList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(List(barbecue, read, broadening, recipes, raichlen, insight, provided, produced, trail, page, gift, horizons, calls, husband, open, culture, cuisine, making, kinds, directions, easy, food, simple, things, love, fun, make, time, yum, interpret),\"Patio_Lawn_and_Garde\",\"0981850006\"\"A2VNYWOPJ13AFP\")\n",
      "(List(side, maneuverability, experimentation, solid, good, distribution, handling, pneumatic, left, cable, crappy, nice, products, feels, flings, arm, long, precise, spreader, farther, give, edgeguard, true, control, material, time, metal, great, settings, tires, bumps),\"Patio_Lawn_and_Garde\",\"B00002N66D\"\"A2E5XXXC07AGA7\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd_text_no_stopwords: org.apache.spark.rdd.RDD[(List[String], String, String)] = MapPartitionsRDD[6] at map at <console>:39\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd_text_no_stopwords = rdd_text_casefolding.map(x => (x._1.filter(!stopWords.contains(_)),x._2,x._3))\n",
    "rdd_text_no_stopwords.take(2).foreach(array => println(stringOf(array)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Get the amount of documents per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_count: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[8] at reduceByKey at <console>:40\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val category_count = rdd.map{ row =>\n",
    "  val json_row = parse(row)\n",
    "  (compact(json_row \\ \"category\").replace(\"\\\"\", \"\"), 1)\n",
    "}.reduceByKey((x,y) => x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Pet_Supplie,1235)\n",
      "(Movies_and_TV,4607)\n",
      "(Electronic,7825)\n",
      "(Office_Product,1243)\n",
      "(Musical_Instrument,500)\n",
      "(Cell_Phones_and_Accessorie,3447)\n",
      "(Clothing_Shoes_and_Jewelry,5749)\n",
      "(Tools_and_Home_Improvement,1926)\n",
      "(Baby,916)\n",
      "(Patio_Lawn_and_Garde,994)\n",
      "(Home_and_Kitche,4254)\n",
      "(Sports_and_Outdoor,3269)\n",
      "(Toys_and_Game,2253)\n",
      "(Digital_Music,836)\n",
      "(Apps_for_Android,2638)\n",
      "(Grocery_and_Gourmet_Food,1297)\n",
      "(Beauty,2023)\n",
      "(Health_and_Personal_Care,2982)\n",
      "(Book,22507)\n",
      "(Automotive,1374)\n",
      "(Kindle_Store,3205)\n",
      "(CDs_and_Vinyl,3749)\n"
     ]
    }
   ],
   "source": [
    "category_count.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "theMap: scala.collection.Map[String,Int] = Map(Kindle_Store -> 3205, Electronic -> 7825, Automotive -> 1374, Pet_Supplie -> 1235, Clothing_Shoes_and_Jewelry -> 5749, Baby -> 916, Grocery_and_Gourmet_Food -> 1297, Musical_Instrument -> 500, Book -> 22507, Movies_and_TV -> 4607, Tools_and_Home_Improvement -> 1926, Sports_and_Outdoor -> 3269, CDs_and_Vinyl -> 3749, Apps_for_Android -> 2638, Home_and_Kitche -> 4254, Office_Product -> 1243, Health_and_Personal_Care -> 2982, Digital_Music -> 836, Cell_Phones_and_Accessorie -> 3447, Beauty -> 2023, Toys_and_Game -> 2253, Patio_Lawn_and_Garde -> 994)\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val theMap =category_count.collectAsMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key = Kindle_Store Value = 3205\n",
      "Key = Electronic Value = 7825\n",
      "Key = Automotive Value = 1374\n",
      "Key = Pet_Supplie Value = 1235\n",
      "Key = Clothing_Shoes_and_Jewelry Value = 5749\n",
      "Key = Baby Value = 916\n",
      "Key = Grocery_and_Gourmet_Food Value = 1297\n",
      "Key = Musical_Instrument Value = 500\n",
      "Key = Book Value = 22507\n",
      "Key = Movies_and_TV Value = 4607\n",
      "Key = Tools_and_Home_Improvement Value = 1926\n",
      "Key = Sports_and_Outdoor Value = 3269\n",
      "Key = CDs_and_Vinyl Value = 3749\n",
      "Key = Apps_for_Android Value = 2638\n",
      "Key = Home_and_Kitche Value = 4254\n",
      "Key = Office_Product Value = 1243\n",
      "Key = Health_and_Personal_Care Value = 2982\n",
      "Key = Digital_Music Value = 836\n",
      "Key = Cell_Phones_and_Accessorie Value = 3447\n",
      "Key = Beauty Value = 2023\n",
      "Key = Toys_and_Game Value = 2253\n",
      "Key = Patio_Lawn_and_Garde Value = 994\n"
     ]
    }
   ],
   "source": [
    " theMap.keys.foreach{ i =>  \n",
    " print( \"Key = \" + i )\n",
    " println(\" Value = \" + theMap(i) )}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Int = 3205\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theMap(\"Kindle_Store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating ChiSquare Values\n",
    "\n",
    "Idea: create rdd tuples like this: ((term, category), 1)\n",
    "then, reduce by key by summing up the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((mezzos,Movies_and_TV),1)\n",
      "((huge,Cell_Phones_and_Accessorie),36)\n",
      "((goodwill,Electronic),2)\n",
      "((lit,Tools_and_Home_Improvement),13)\n",
      "((alittle,Health_and_Personal_Care),2)\n",
      "((aspca,Pet_Supplie),1)\n",
      "((alwasy,Book),1)\n",
      "((scales,Digital_Music),1)\n",
      "((instructor,Movies_and_TV),11)\n",
      "((upright,Automotive),2)\n",
      "((personally,Apps_for_Android),5)\n",
      "((absorb,Baby),4)\n",
      "((repopulate,Book),1)\n",
      "((perfect,Movies_and_TV),194)\n",
      "((rib,Kindle_Store),1)\n",
      "((tubular,Health_and_Personal_Care),1)\n",
      "((ten,Patio_Lawn_and_Garde),4)\n",
      "((duet,Musical_Instrument),2)\n",
      "((stripped,Electronic),5)\n",
      "((trevor's,Book),1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "convert: ((List[String], String)) => List[((String, String), Int)] = <function1>\r\n",
       "chiSq1: org.apache.spark.rdd.RDD[((String, String), Int)] = ShuffledRDD[11] at reduceByKey at <console>:41\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//main idea: https://stackoverflow.com/questions/39005801/mapvalues-and-explode-in-rdd\n",
    "val convert: ((List[String], String)) => List[((String, String), Int)] = tuple => tuple._1.map(term =>\n",
    "  ((term, tuple._2.replace(\"\\\"\", \"\")), 1)).toList\n",
    "\n",
    "val chiSq1 = rdd_text_no_stopwords.map(x => (x._1, x._2)).flatMap(convert).reduceByKey((x,y) => x + y)\n",
    "chiSq1.take(20).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(mezzos,(Movies_and_TV,1))\n",
      "(huge,(Cell_Phones_and_Accessorie,36))\n",
      "(goodwill,(Electronic,2))\n",
      "(lit,(Tools_and_Home_Improvement,13))\n",
      "(alittle,(Health_and_Personal_Care,2))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "chiSq2: org.apache.spark.rdd.RDD[(String, (String, Int))] = MapPartitionsRDD[12] at map at <console>:37\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val chiSq2 = chiSq1.map(x => (x._1._1, (x._1._2, x._2)))\n",
    "chiSq2.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(vecindad,CompactBuffer((Movies_and_TV,1)))\n",
      "(bone,CompactBuffer((Pet_Supplie,9), (Home_and_Kitche,3), (Cell_Phones_and_Accessorie,3), (Sports_and_Outdoor,5), (Toys_and_Game,1), (CDs_and_Vinyl,11), (Book,26), (Tools_and_Home_Improvement,1), (Digital_Music,1), (Automotive,1), (Kindle_Store,2), (Beauty,2), (Musical_Instrument,1), (Patio_Lawn_and_Garde,1), (Movies_and_TV,9), (Clothing_Shoes_and_Jewelry,4), (Health_and_Personal_Care,13)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "chiSq3: org.apache.spark.rdd.RDD[(String, Iterable[(String, Int)])] = ShuffledRDD[13] at groupByKey at <console>:37\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val chiSq3 = chiSq2.groupByKey()\n",
    "chiSq3.take(2).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chiSq4: org.apache.spark.rdd.RDD[(String, scala.collection.immutable.Map[String,List[Int]])] = MapPartitionsRDD[15] at map at <console>:37\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val chiSq4 = chiSq3.map(x => (x._1, x._2.toList)).map(x => (x._1, x._2.groupBy(_._1).map { case (k,v) => (k,v.map(_._2))}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(vecindad,Map(Movies_and_TV -> List(1)))\n",
      "(bone,Map(Patio_Lawn_and_Garde -> List(1), Movies_and_TV -> List(9), Tools_and_Home_Improvement -> List(1), Kindle_Store -> List(2), Home_and_Kitche -> List(3), Digital_Music -> List(1), Automotive -> List(1), Book -> List(26), Clothing_Shoes_and_Jewelry -> List(4), Toys_and_Game -> List(1), Health_and_Personal_Care -> List(13), Sports_and_Outdoor -> List(5), Beauty -> List(2), CDs_and_Vinyl -> List(11), Musical_Instrument -> List(1), Cell_Phones_and_Accessorie -> List(3), Pet_Supplie -> List(9)))\n",
      "(glorifying,Map(Book -> List(4), Movies_and_TV -> List(1)))\n",
      "(rollon,Map(Beauty -> List(1)))\n",
      "(folan,Map(Movies_and_TV -> List(1)))\n"
     ]
    }
   ],
   "source": [
    "chiSq4.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testlist: List[(String, Int)] = List((a,1), (b,2))\r\n",
       "testdict: scala.collection.immutable.Map[String,List[Int]] = Map(b -> List(2), a -> List(1))\n"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testlist = (\"a\", 1) :: (\"b\", 2) :: Nil\n",
    "val testdict = testlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res153: Iterable[String] = Set(b, a)\n"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdict.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chiSqlCalc: (t: (String, Map[String,List[Int]]))(String, Map[String,List[Int]])\r\n",
       "ttuple: (String, scala.collection.immutable.Map[String,List[Int]]) = (vecindad,Map(Movies_and_TV -> List(1)))\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chiSqlCalc (t: (String, Map[String, List[Int]])): (String, Map[String, List[Int]]) = {\n",
    "        t\n",
    "}\n",
    "val ttuple = (\"vecindad\",Map(\"Movies_and_TV\" -> List(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res_ttuple: (String, Map[String,List[Int]]) = (vecindad,Map(Movies_and_TV -> List(1)))\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val res_ttuple = chiSqlCalc(ttuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(vecindad,Map(Movies_and_TV -> List(1)))\n",
      "(bone,Map(Patio_Lawn_and_Garde -> List(1), Movies_and_TV -> List(9), Tools_and_Home_Improvement -> List(1), Kindle_Store -> List(2), Home_and_Kitche -> List(3), Digital_Music -> List(1), Automotive -> List(1), Book -> List(26), Clothing_Shoes_and_Jewelry -> List(4), Toys_and_Game -> List(1), Health_and_Personal_Care -> List(13), Sports_and_Outdoor -> List(5), Beauty -> List(2), CDs_and_Vinyl -> List(11), Musical_Instrument -> List(1), Cell_Phones_and_Accessorie -> List(3), Pet_Supplie -> List(9)))\n",
      "(glorifying,Map(Book -> List(4), Movies_and_TV -> List(1)))\n",
      "(rollon,Map(Beauty -> List(1)))\n",
      "(folan,Map(Movies_and_TV -> List(1)))\n"
     ]
    }
   ],
   "source": [
    "chiSq4.map(x => chiSqlCalc(x)).take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    " skeleton function to ChiSq:\n",
    " \n",
    " def chiSqlCalc (t: (String, Map[String, List[Int]])): (String, Map[String, List[Int]]) = {\n",
    "        t\n",
    "}\n",
    "\n",
    "chiSq4.map(x => chiSqlCalc(x)).take(5).foreach(println)\n",
    "*/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
